{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-j955t3nUEE",
        "outputId": "f4685b35-19a5-475f-81a2-65ca87758bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.9/543.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q lime==0.2.0.1 shap==0.46.0 \\\n",
        "               librosa==0.10.2.post1 soundfile==0.12.1 \\\n",
        "               scikit-learn==1.6.0 matplotlib==3.9.0 torchsummary==1.5.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alzheimer's Detection (100-per-class train) + Explainability + AUC-ROC + Training Metrics\n",
        "import os, random, warnings\n",
        "import numpy as np\n",
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/drive/MyDrive/Alzheimers_Organized\"\n",
        "control_path = os.path.join(data_dir, \"control\")\n",
        "dementia_path = os.path.join(data_dir, \"dementia\")\n",
        "\n",
        "\n",
        "# Audio Feature Extraction Utilities\n",
        "def extract_audio_features(audio_path, sr=22050):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20).T, axis=0)\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
        "        centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "        bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "        rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "        rms = np.mean(librosa.feature.rms(y=y))\n",
        "        zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))\n",
        "        return np.hstack([mfccs, chroma, contrast, [centroid, bandwidth, rolloff, rms, zcr]])\n",
        "    except Exception:\n",
        "        return np.zeros(44, dtype=float)\n",
        "\n",
        "\n",
        "def collect_wavs_and_texts(root_dir):\n",
        "    pairs = []\n",
        "    for subj in os.listdir(root_dir):\n",
        "        subj_dir = os.path.join(root_dir, subj)\n",
        "        if not os.path.isdir(subj_dir):\n",
        "            continue\n",
        "        wavs = [w for w in os.listdir(subj_dir) if w.lower().endswith(\".wav\")]\n",
        "        for w in wavs:\n",
        "            wav_path = os.path.join(subj_dir, w)\n",
        "            txt_path = wav_path[:-4] + \".txt\"\n",
        "            text = \"\"\n",
        "            if os.path.exists(txt_path):\n",
        "                try:\n",
        "                    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                        text = f.read().lower()\n",
        "                except:\n",
        "                    text = \"\"\n",
        "            pairs.append((wav_path, text))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def build_features(pairs):\n",
        "    feats, texts = [], []\n",
        "    for wav_path, text in tqdm(pairs, desc=\"Extracting audio features\"):\n",
        "        feats.append(extract_audio_features(wav_path))\n",
        "        texts.append(text)\n",
        "    return np.array(feats), texts\n",
        "\n",
        "\n",
        "def save_spectrogram(audio_path, save_path, title):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(8, 3.5))\n",
        "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format=\"%+2.0f dB\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "random.seed(42)\n",
        "\n",
        "control_pairs = collect_wavs_and_texts(control_path)\n",
        "dementia_pairs = collect_wavs_and_texts(dementia_path)\n",
        "\n",
        "print(f\"Control files:  {len(control_pairs)}\")\n",
        "print(f\"Dementia files: {len(dementia_pairs)}\")\n",
        "\n",
        "random.shuffle(control_pairs)\n",
        "random.shuffle(dementia_pairs)\n",
        "\n",
        "K = 100\n",
        "control_train = control_pairs[:min(K, len(control_pairs))]\n",
        "control_unseen = control_pairs[min(K, len(control_pairs)):]\n",
        "dementia_train = dementia_pairs[:min(K, len(dementia_pairs))]\n",
        "dementia_unseen = dementia_pairs[min(K, len(dementia_pairs)):]\n",
        "\n",
        "print(f\"Control: train={len(control_train)}, unseen={len(control_unseen)}\")\n",
        "print(f\"Dementia: train={len(dementia_train)}, unseen={len(dementia_unseen)}\")\n",
        "\n",
        "# Extract audio + text\n",
        "X_audio_train_control, texts_train_control = build_features(control_train)\n",
        "X_audio_train_dementia, texts_train_dementia = build_features(dementia_train)\n",
        "X_audio_unseen_control, texts_unseen_control = build_features(control_unseen)\n",
        "X_audio_unseen_dementia, texts_unseen_dementia = build_features(dementia_unseen)\n",
        "\n",
        "X_audio_train = np.vstack([X_audio_train_control, X_audio_train_dementia])\n",
        "y_train = np.array([0]*len(X_audio_train_control) + [1]*len(X_audio_train_dementia))\n",
        "texts_train = texts_train_control + texts_train_dementia\n",
        "\n",
        "X_audio_unseen = np.vstack([X_audio_unseen_control, X_audio_unseen_dementia])\n",
        "y_unseen = np.array([0]*len(X_audio_unseen_control) + [1]*len(X_audio_unseen_dementia))\n",
        "texts_unseen = texts_unseen_control + texts_unseen_dementia\n",
        "\n",
        "print(f\"Train: {X_audio_train.shape}, Unseen: {X_audio_unseen.shape}\")\n",
        "\n",
        "# TF-IDF text features\n",
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "X_text_train = vectorizer.fit_transform(texts_train).toarray()\n",
        "X_text_unseen = vectorizer.transform(texts_unseen).toarray()\n",
        "\n",
        "# Combine audio + text features\n",
        "X_train = np.hstack([X_audio_train, X_text_train])\n",
        "X_unseen = np.hstack([X_audio_unseen, X_text_unseen])\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_unseen = scaler.transform(X_unseen)\n",
        "\n",
        "# Model Training & Evaluation (No LightGBM)\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM (RBF)\": SVC(kernel='rbf', probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "}\n",
        "\n",
        "explain_dir = \"/content/drive/MyDrive/Alzheimers_Explainability\"\n",
        "os.makedirs(explain_dir, exist_ok=True)\n",
        "\n",
        "history = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(\"\\nModel:\", name)\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    train_preds = model.predict(X_tr)\n",
        "    val_preds = model.predict(X_val)\n",
        "    test_preds = model.predict(X_unseen)\n",
        "    test_probs = model.predict_proba(X_unseen)[:,1] if hasattr(model, \"predict_proba\") else np.zeros(len(test_preds))\n",
        "\n",
        "    train_acc = accuracy_score(y_tr, train_preds)\n",
        "    val_acc = accuracy_score(y_val, val_preds)\n",
        "    test_acc = accuracy_score(y_unseen, test_preds)\n",
        "    auc = roc_auc_score(y_unseen, test_probs) if len(np.unique(y_unseen)) > 1 else np.nan\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc:.3f}\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.3f}\")\n",
        "    print(f\"Testing Accuracy: {test_acc:.3f}\")\n",
        "    print(f\"AUC-ROC: {auc:.3f}\")\n",
        "\n",
        "    if len(np.unique(y_unseen)) > 1:\n",
        "        fpr, tpr, _ = roc_curve(y_unseen, test_probs)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.2f})\")\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"AUC-ROC Curve: {name}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(explain_dir, f\"auc_{name.replace(' ', '_')}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    history[name] = {\"train_acc\": train_acc, \"val_acc\": val_acc}\n",
        "\n",
        "# Accuracy Visualization\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(range(len(history)), [history[m][\"train_acc\"] for m in history], width=0.4, label=\"Training Accuracy\")\n",
        "plt.bar(np.arange(len(history)) + 0.4, [history[m][\"val_acc\"] for m in history], width=0.4, label=\"Validation Accuracy\")\n",
        "plt.xticks(np.arange(len(history)) + 0.2, list(history.keys()), rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(explain_dir, \"train_val_accuracy.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Explainability Section (LIME + SHAP + Spectrogram)\n",
        "ctrl_example = control_unseen[0][0] if control_unseen else control_train[0][0]\n",
        "dem_example = dementia_unseen[0][0] if dementia_unseen else dementia_train[0][0]\n",
        "\n",
        "if ctrl_example:\n",
        "    save_spectrogram(ctrl_example, os.path.join(explain_dir, \"spectrogram_control.png\"), \"Mel Spectrogram (Control)\")\n",
        "if dem_example:\n",
        "    save_spectrogram(dem_example, os.path.join(explain_dir, \"spectrogram_dementia.png\"), \"Mel Spectrogram (Dementia)\")\n",
        "\n",
        "# LIME for Text\n",
        "text_pipeline = make_pipeline(TfidfVectorizer(max_features=300), LogisticRegression(max_iter=1000))\n",
        "text_pipeline.fit(texts_train, y_train)\n",
        "\n",
        "lime_explainer = LimeTextExplainer(class_names=[\"Control\", \"Dementia\"])\n",
        "sample_text = texts_unseen[0] if len(texts_unseen) > 0 else texts_train[0]\n",
        "lime_exp = lime_explainer.explain_instance(sample_text, text_pipeline.predict_proba, num_features=10)\n",
        "lime_exp.save_to_file(os.path.join(explain_dir, \"lime_text_explanation.html\"))\n",
        "\n",
        "# SHAP for Text\n",
        "tfidf = text_pipeline.named_steps[\"tfidfvectorizer\"]\n",
        "logreg = text_pipeline.named_steps[\"logisticregression\"]\n",
        "X_bg = tfidf.transform(texts_train[:100])\n",
        "explainer = shap.LinearExplainer(logreg, X_bg, feature_perturbation=\"interventional\")\n",
        "\n",
        "X_sample = tfidf.transform([sample_text])\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "vals = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
        "vals = np.array(vals).ravel()\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "idx = np.argsort(np.abs(vals))[-15:][::-1]\n",
        "top_features = [feature_names[i] for i in idx]\n",
        "top_values = vals[idx]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(range(len(top_features))[::-1], top_values[np.argsort(np.arange(len(top_features))[::-1])])\n",
        "plt.yticks(range(len(top_features))[::-1], top_features[::-1])\n",
        "plt.xlabel(\"SHAP value (word contribution)\")\n",
        "plt.title(\"Top word contributions (SHAP: LogisticRegression + TF-IDF)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(explain_dir, \"shap_text_bar.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"Explainability visualizations and evaluation results saved in Google Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG1r8JHenU7q",
        "outputId": "f4a9f833-bc1a-4e65-c36b-159ebaa6587c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Control files:  237\n",
            "Dementia files: 131\n",
            "Control: train=100, unseen=137\n",
            "Dementia: train=100, unseen=31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting audio features: 100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n",
            "Extracting audio features: 100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n",
            "Extracting audio features: 100%|██████████| 137/137 [02:32<00:00,  1.11s/it]\n",
            "Extracting audio features: 100%|██████████| 31/31 [00:46<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (200, 44), Unseen: (168, 44)\n",
            "\n",
            "Model: Logistic Regression\n",
            "Training Accuracy: 1.000\n",
            "Validation Accuracy: 0.450\n",
            "Testing Accuracy: 0.506\n",
            "AUC-ROC: 0.518\n",
            "\n",
            "Model: SVM (RBF)\n",
            "Training Accuracy: 0.994\n",
            "Validation Accuracy: 0.475\n",
            "Testing Accuracy: 0.625\n",
            "AUC-ROC: 0.655\n",
            "\n",
            "Model: Random Forest\n",
            "Training Accuracy: 1.000\n",
            "Validation Accuracy: 0.525\n",
            "Testing Accuracy: 0.601\n",
            "AUC-ROC: 0.700\n",
            "\n",
            "Model: KNN\n",
            "Training Accuracy: 0.556\n",
            "Validation Accuracy: 0.500\n",
            "Testing Accuracy: 0.202\n",
            "AUC-ROC: 0.444\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Training Accuracy: 1.000\n",
            "Validation Accuracy: 0.525\n",
            "Testing Accuracy: 0.518\n",
            "AUC-ROC: 0.578\n",
            "\n",
            "Model: XGBoost\n",
            "Training Accuracy: 1.000\n",
            "Validation Accuracy: 0.575\n",
            "Testing Accuracy: 0.673\n",
            "AUC-ROC: 0.670\n",
            "Explainability visualizations and evaluation results saved in Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Calculate training and validation losses for all trained models\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "model_names = list(models.keys())\n",
        "\n",
        "for name, model in models.items():\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Check if model supports probability output\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        train_probs = model.predict_proba(X_tr)\n",
        "        val_probs = model.predict_proba(X_val)\n",
        "        tr_loss = log_loss(y_tr, train_probs)\n",
        "        va_loss = log_loss(y_val, val_probs)\n",
        "    else:\n",
        "        # Approximate using predictions (for SVM or models without predict_proba)\n",
        "        tr_preds = model.predict(X_tr)\n",
        "        va_preds = model.predict(X_val)\n",
        "        tr_loss = np.mean(tr_preds != y_tr)\n",
        "        va_loss = np.mean(va_preds != y_val)\n",
        "\n",
        "    train_losses.append(tr_loss)\n",
        "    val_losses.append(va_loss)\n",
        "    print(f\"{name}: Training Loss = {tr_loss:.4f}, Validation Loss = {va_loss:.4f}\")\n",
        "\n",
        "# Plot the Training vs Validation Loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(model_names, train_losses, marker='o', label='Training Loss')\n",
        "plt.plot(model_names, val_losses, marker='s', label='Validation Loss')\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Loss (Cross-Entropy / Proxy)\")\n",
        "plt.title(\"Training vs Validation Loss Comparison\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(explain_dir, \"train_val_loss.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"Training and validation loss plot saved as 'train_val_loss.png' in the Explainability folder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eDsQvTtuyfH",
        "outputId": "af19a421-b90d-488a-feee-283fb008a00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: Training Loss = 0.0214, Validation Loss = 1.8780\n",
            "SVM (RBF): Training Loss = 0.2753, Validation Loss = 0.6829\n",
            "Random Forest: Training Loss = 0.2018, Validation Loss = 0.6886\n",
            "KNN: Training Loss = 0.5820, Validation Loss = 0.9006\n",
            "Gradient Boosting: Training Loss = 0.0744, Validation Loss = 0.7839\n",
            "XGBoost: Training Loss = 0.0201, Validation Loss = 0.9382\n",
            "Training and validation loss plot saved as 'train_val_loss.png' in the Explainability folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4VGO2p7Zn3qq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}